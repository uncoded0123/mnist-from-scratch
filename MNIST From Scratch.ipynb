{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e691311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1750, 0.2277, 0.3016, 0.0830, 0.0658, 0.0972, 0.0368, 1.0000, 0.1991,\n",
       "          0.3058],\n",
       "         [0.0137, 0.1028, 0.0272, 0.0289, 0.0408, 0.0065, 0.0177, 1.0000, 0.0430,\n",
       "          0.0962],\n",
       "         [0.0082, 0.2907, 0.0621, 0.0184, 0.0581, 0.0418, 0.0032, 1.0000, 0.0352,\n",
       "          0.4747],\n",
       "         [0.0217, 0.1063, 0.2089, 0.0263, 0.0736, 0.0212, 0.0322, 0.1344, 0.0767,\n",
       "          1.0000],\n",
       "         [0.0036, 0.1693, 1.0000, 0.0578, 0.0109, 0.0122, 0.0042, 0.1375, 0.0307,\n",
       "          0.5769],\n",
       "         [0.0270, 0.5638, 0.3562, 0.0251, 1.0000, 0.7574, 0.0155, 0.7883, 0.0283,\n",
       "          0.5515],\n",
       "         [0.1375, 0.4880, 0.0648, 0.0688, 0.0642, 0.0407, 0.0683, 1.0000, 0.0865,\n",
       "          0.5055],\n",
       "         [0.0532, 0.5122, 0.0265, 0.0493, 0.0348, 0.0553, 0.0356, 0.1422, 0.3626,\n",
       "          1.0000],\n",
       "         [0.0335, 0.0502, 0.0028, 0.0117, 0.0372, 0.0062, 0.0020, 0.3779, 0.0087,\n",
       "          1.0000],\n",
       "         [0.0639, 0.0527, 0.0853, 0.0488, 0.0478, 0.0153, 0.0106, 0.5025, 0.0777,\n",
       "          1.0000],\n",
       "         [0.1993, 0.4275, 0.2501, 0.3092, 0.1145, 0.1297, 0.1130, 0.5025, 0.3941,\n",
       "          1.0000],\n",
       "         [0.0115, 0.4079, 0.1257, 0.0161, 1.0000, 0.6541, 0.0268, 0.6353, 0.0198,\n",
       "          0.1979],\n",
       "         [0.3730, 0.6279, 0.5581, 0.2342, 0.4237, 0.0595, 0.3224, 0.7922, 1.0000,\n",
       "          0.5179],\n",
       "         [0.0253, 0.7171, 0.0760, 0.0241, 0.0672, 0.0353, 0.0087, 1.0000, 0.0348,\n",
       "          0.1460],\n",
       "         [0.0208, 0.2331, 0.1030, 0.0291, 0.1261, 0.0064, 0.0358, 1.0000, 0.0953,\n",
       "          0.1725],\n",
       "         [0.0289, 0.0641, 0.1641, 0.0465, 0.0603, 0.0140, 0.0091, 0.6483, 0.1137,\n",
       "          1.0000],\n",
       "         [0.1960, 0.8672, 0.1031, 0.0560, 0.2304, 0.1745, 0.0562, 1.0000, 0.1735,\n",
       "          0.6049],\n",
       "         [0.0243, 0.0973, 0.0209, 0.0680, 0.0781, 0.0207, 0.0104, 0.3292, 0.2294,\n",
       "          1.0000],\n",
       "         [0.0065, 0.0167, 0.0350, 0.0067, 0.0057, 0.0070, 0.0022, 0.1377, 0.0070,\n",
       "          1.0000],\n",
       "         [0.1261, 0.2256, 0.0772, 0.0990, 0.0812, 0.0411, 0.0072, 0.8415, 0.0381,\n",
       "          1.0000],\n",
       "         [0.2550, 0.6033, 0.1945, 0.1003, 0.1831, 0.2798, 0.0544, 0.7933, 0.1006,\n",
       "          1.0000],\n",
       "         [0.0410, 0.2530, 0.0490, 0.0558, 0.0145, 0.0468, 0.0081, 0.3453, 0.0211,\n",
       "          1.0000],\n",
       "         [0.0375, 0.2170, 0.1499, 0.1422, 0.2254, 0.0485, 0.1189, 1.0000, 0.3505,\n",
       "          0.7397],\n",
       "         [0.1757, 0.1914, 0.4675, 0.1073, 0.0238, 0.1406, 0.0533, 0.8508, 0.1427,\n",
       "          1.0000],\n",
       "         [1.0000, 0.3791, 0.5324, 0.8768, 0.0940, 0.0986, 0.0480, 0.7679, 0.1969,\n",
       "          0.4191],\n",
       "         [0.0069, 0.3148, 0.0517, 0.0124, 0.6644, 0.0446, 0.0151, 0.6805, 0.0328,\n",
       "          1.0000],\n",
       "         [0.2251, 0.1882, 0.0173, 0.0711, 0.0311, 0.0143, 0.0244, 1.0000, 0.1478,\n",
       "          0.6443],\n",
       "         [0.0584, 0.0980, 0.0198, 0.0272, 0.0486, 0.0154, 0.0248, 1.0000, 0.0299,\n",
       "          0.1775],\n",
       "         [0.0034, 0.0348, 0.0147, 0.0048, 0.0187, 0.0025, 0.0069, 1.0000, 0.0405,\n",
       "          0.1099],\n",
       "         [0.0318, 0.1774, 0.3272, 0.0160, 0.0425, 0.0289, 0.0071, 1.0000, 0.0335,\n",
       "          0.3197],\n",
       "         [0.0313, 1.0000, 0.1102, 0.0630, 0.0591, 0.0618, 0.0063, 0.2270, 0.0156,\n",
       "          0.0951],\n",
       "         [0.1402, 0.4846, 0.3627, 0.1365, 0.1232, 0.1176, 0.2630, 1.0000, 0.2397,\n",
       "          0.2650]], grad_fn=<PowBackward2>),\n",
       " tensor(2.7652, grad_fn=<NegBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "\n",
    "\n",
    "# ------------------- Get Data -------------------\n",
    "# 60,000 samples, 28x28 pixels / sample\n",
    "train_dataset = torchvision.datasets.MNIST(train=True, download=True, root='./')\n",
    "train_rand = torch.randperm(train_dataset.data.shape[0]) # for picking random images/labels, like randint except has no repeats\n",
    "train_img_tensors = train_dataset.data.reshape(-1,784)[train_rand]\n",
    "Y = train_dataset.targets[train_rand]\n",
    "\n",
    "# 10,000 samples, 28x28 pixels / sample\n",
    "test_dataset = torchvision.datasets.MNIST(train=False, download=True, root='./')\n",
    "test_rand = torch.randperm(test_dataset.data.shape[0]) # for picking random images/labels, like randint except has no repeats\n",
    "test_img_tensors = test_dataset.data.reshape(-1,784)[test_rand]\n",
    "test_Y = test_dataset.targets[test_rand]\n",
    "\n",
    "\n",
    "# ------------------- Train -------------------\n",
    "zero = torch.tensor(0)\n",
    "batch_size = 32\n",
    "# input nodes batch\n",
    "# 32 x 784\n",
    "X = train_img_tensors[:batch_size].float() / 255\n",
    "\n",
    "h1_nodes = 128\n",
    "w1 = torch.randn((h1_nodes, 784)).requires_grad_(True)\n",
    "w1 = w1 / h1_nodes**0.5\n",
    "l1 = X@w1.T\n",
    "relu1 = torch.maximum(zero, l1)\n",
    "\n",
    "h2_nodes = 64\n",
    "w2 = torch.randn((h2_nodes, h1_nodes)).requires_grad_(True)\n",
    "w2 = w2 / h2_nodes**0.5\n",
    "l2 = relu1@w2.T\n",
    "relu2 = torch.maximum(zero, l2)\n",
    "\n",
    "out_nodes = 10\n",
    "w3 = torch.randn((out_nodes, h2_nodes)).requires_grad_(True)\n",
    "w3 = w3 / out_nodes**0.5\n",
    "logits = relu2@w3.T\n",
    "\n",
    "logit_maxes = logits.max(dim=1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes\n",
    "\n",
    "e_to_the_logits = torch.e**norm_logits\n",
    "e_to_the_logits_sum = torch.sum(e_to_the_logits, dim=1, keepdim=True)\n",
    "e_to_the_logits_sum_inv = e_to_the_logits_sum**-1\n",
    "probs = e_to_the_logits * e_to_the_logits_sum_inv # prob = softmax. softmax is an activation function, as is relu\n",
    "\n",
    "\n",
    "logprobs = probs.log()\n",
    "# ------------------- Loss -------------------\n",
    "\n",
    "# understand this before moving on to anything else\n",
    "# grabs predictions for 32 images, and correct values in each image\n",
    "loss = -logprobs[range(batch_size), Y[:batch_size]].mean()\n",
    "loss\n",
    "\n",
    "# ------------------- Back -------------------\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(batch_size), Y[:batch_size]] = -1/batch_size\n",
    "dprobs = (1 / probs) * dlogprobs\n",
    "de_to_the_logits = e_to_the_logits_sum_inv * dprobs\n",
    "de_to_the_logits_sum_inv = (e_to_the_logits * dprobs).sum(dim=1, keepdim=True)\n",
    "de_to_the_logits_sum = -e_to_the_logits_sum**(-2) * de_to_the_logits_sum_inv\n",
    "de_to_the_logits = de_to_the_logits + de_to_the_logits_sum\n",
    "dlogits = torch.e**logits * de_to_the_logits\n",
    "drelu2 = dlogits @ w3\n",
    "dw3 = dlogits.T @ relu2\n",
    "dl2 = (l2 > 0) * drelu2\n",
    "dw2 = dl2.T @ relu1\n",
    "drelu1 = dl2 @ w2\n",
    "dl1 = (l1 > 0) * drelu1\n",
    "dw1 = dl1.T @ X\n",
    "e_to_the_logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bed7df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5051/3845677963.py:5: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(torch.allclose(dw1, w1.grad, atol=1e-4))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "allclose(): argument 'other' (position 2) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Compare your gradients vs PyTorch's\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mallclose(dw1, w1\u001b[38;5;241m.\u001b[39mgrad, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mallclose(dw2, w2\u001b[38;5;241m.\u001b[39mgrad, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mallclose(dw3, w3\u001b[38;5;241m.\u001b[39mgrad, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: allclose(): argument 'other' (position 2) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "# After loss computed\n",
    "loss.backward()\n",
    "\n",
    "# Compare your gradients vs PyTorch's\n",
    "print(torch.allclose(dw1, w1.grad, atol=1e-4))\n",
    "print(torch.allclose(dw2, w2.grad, atol=1e-4))\n",
    "print(torch.allclose(dw3, w3.grad, atol=1e-4))\n",
    "# print(torch.allclose(dw1, w1.grad, atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bab7946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "tensor(nan, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print((dw1 - w1.grad).abs().max())\n",
    "print((dw2 - w2.grad).abs().max())\n",
    "print((dw3 - w3.grad).abs().max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
